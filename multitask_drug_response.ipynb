{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Multi-Task Drug Response Prediction\n\n**Goal**: Predict IC50 values for multiple drugs simultaneously\n\n**Why Multi-Task?**\n- Single-drug prediction can overfit to drug-specific patterns\n- Multi-task forces the model to learn generalizable biological features\n- More realistic evaluation: can the model predict drug responses broadly?\n\n**Problem Difficulty Options**:\n1. **Random split** (Baseline): Standard 80/20 random split\n2. **Histology-based split** (Hard): Test on unseen cancer subtypes\n3. **Site-based split** (Hard): Test on unseen tissue origins"
  },
  {
   "cell_type": "markdown",
   "id": "colab-setup-header",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "Run the cell below first to set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = '/content/drive/MyDrive/bioai_data'\n",
    "    os.chdir(DATA_DIR)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    print(f\"Files available: {os.listdir('.')}\")\n",
    "else:\n",
    "    print(\"Not running in Colab - using local paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-omics feature data\n",
    "methexpr_df = pd.read_csv('ml_with_gene_expr.csv.gz',\n",
    "                          compression='gzip',\n",
    "                          index_col=0,\n",
    "                          low_memory=False)\n",
    "\n",
    "# Separate feature types\n",
    "metadata_cols = ['primary site', 'primary histology', 'cosmic_id']\n",
    "methylation_cols = [col for col in methexpr_df.columns if col.startswith('cg')]\n",
    "expression_cols = [col for col in methexpr_df.columns if col.startswith('expr_')]\n",
    "\n",
    "print(f\"Feature data shape: {methexpr_df.shape}\")\n",
    "print(f\"Methylation features: {len(methylation_cols)}\")\n",
    "print(f\"Expression features: {len(expression_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-drug-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load drug response data\n",
    "response_df = pd.read_csv('ML_dataset_methylation_drug_response.csv.gz',\n",
    "                          compression='gzip',\n",
    "                          index_col=0,\n",
    "                          low_memory=False)\n",
    "\n",
    "# Extract only drug columns (not methylation features or metadata)\n",
    "drug_cols = [col for col in response_df.columns \n",
    "             if not col.startswith('cg') and col not in metadata_cols]\n",
    "\n",
    "print(f\"Drug response data shape: {response_df.shape}\")\n",
    "print(f\"Total drugs available: {len(drug_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drug-selection-header",
   "metadata": {},
   "source": [
    "## Drug Selection by Coverage\n",
    "\n",
    "Select drugs with high coverage (most cell lines have IC50 values) to minimize missing data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze drug coverage\n",
    "drug_response = response_df[drug_cols]\n",
    "coverage = drug_response.notna().sum().sort_values(ascending=False)\n",
    "total_samples = len(response_df)\n",
    "\n",
    "print(f\"Drug coverage statistics (of {total_samples} cell lines):\")\n",
    "print(f\"  Max coverage: {coverage.max()} ({100*coverage.max()/total_samples:.1f}%)\")\n",
    "print(f\"  Min coverage: {coverage.min()} ({100*coverage.min()/total_samples:.1f}%)\")\n",
    "print(f\"  Median coverage: {coverage.median():.0f} ({100*coverage.median()/total_samples:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDrugs by coverage threshold:\")\n",
    "for thresh in [0.95, 0.90, 0.85, 0.80]:\n",
    "    n_drugs = (coverage > thresh * total_samples).sum()\n",
    "    print(f\"  >{thresh*100:.0f}% coverage: {n_drugs} drugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-drugs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select drugs with high coverage\n",
    "# CONFIGURABLE: Adjust threshold to balance number of drugs vs missing data\n",
    "COVERAGE_THRESHOLD = 0.90  # 90% coverage = 288 drugs\n",
    "\n",
    "min_samples = int(COVERAGE_THRESHOLD * total_samples)\n",
    "selected_drugs = coverage[coverage >= min_samples].index.tolist()\n",
    "\n",
    "print(f\"Coverage threshold: {COVERAGE_THRESHOLD*100:.0f}% ({min_samples} samples minimum)\")\n",
    "print(f\"Selected drugs: {len(selected_drugs)}\")\n",
    "print(f\"\\nTop 10 drugs by coverage:\")\n",
    "for drug in selected_drugs[:10]:\n",
    "    print(f\"  {drug}: {coverage[drug]} ({100*coverage[drug]/total_samples:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-header",
   "metadata": {},
   "source": [
    "## Merge Features and Drug Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge feature data with drug responses (inner join on cell line names)\n",
    "df = methexpr_df.join(response_df[selected_drugs], how='inner')\n",
    "\n",
    "print(f\"Merged dataset shape: {df.shape}\")\n",
    "print(f\"Cell lines with both features and drug data: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-multitask-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare multi-task target matrix (Y)\n",
    "Y_all = df[selected_drugs]\n",
    "\n",
    "# Find cell lines with complete drug response data (no missing values)\n",
    "complete_mask = Y_all.notna().all(axis=1)\n",
    "n_complete = complete_mask.sum()\n",
    "\n",
    "print(f\"Cell lines with complete drug data: {n_complete} ({100*n_complete/len(df):.1f}%)\")\n",
    "\n",
    "# Alternative: use cell lines with >X% drug coverage\n",
    "CELL_COVERAGE_THRESHOLD = 0.95\n",
    "cell_coverage = Y_all.notna().sum(axis=1) / len(selected_drugs)\n",
    "usable_cells = cell_coverage >= CELL_COVERAGE_THRESHOLD\n",
    "\n",
    "print(f\"Cell lines with >{CELL_COVERAGE_THRESHOLD*100:.0f}% drug coverage: {usable_cells.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-dataset",
   "metadata": {},
   "outputs": [],
   "source": "# Use cell lines with high drug coverage, impute remaining missing values\ndf_filtered = df[usable_cells].copy()\n\n# Prepare features (X) and targets (Y)\nfeature_cols = methylation_cols + expression_cols\nX = df_filtered[feature_cols]\n\n# Drop columns with any NaN (matches harder_ds.ipynb approach)\n# This removes ~336 methylation probes with technical failures\nX = X.dropna(axis=1)\n\nY = df_filtered[selected_drugs]\n\n# Impute remaining missing drug values with drug-specific median\nY_imputed = Y.fillna(Y.median())\n\nprint(f\"Final dataset:\")\nprint(f\"  X (features): {X.shape}\")\nprint(f\"  Y (drug responses): {Y_imputed.shape}\")\nprint(f\"  Dropped {len(feature_cols) - X.shape[1]} features with NaN (matching harder_ds.ipynb)\")\nprint(f\"  Missing values in Y after imputation: {Y_imputed.isna().sum().sum()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "0wvywj7v35v",
   "source": "## Multi-Task Evaluation Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2lbcwa3ir7c",
   "source": "def evaluate_multitask(model, X_train, Y_train, X_test, Y_test):\n    \"\"\"\n    Evaluate multi-output regression model.\n    \n    Returns:\n        dict with per-drug and aggregate metrics\n    \"\"\"\n    # Predictions\n    Y_pred_train = model.predict(X_train)\n    Y_pred_test = model.predict(X_test)\n    \n    # Convert to DataFrame for easier analysis\n    Y_pred_train_df = pd.DataFrame(Y_pred_train, columns=Y_train.columns, index=Y_train.index)\n    Y_pred_test_df = pd.DataFrame(Y_pred_test, columns=Y_test.columns, index=Y_test.index)\n    \n    # Per-drug metrics\n    train_r2_per_drug = []\n    test_r2_per_drug = []\n    test_mse_per_drug = []\n    \n    for drug in Y_train.columns:\n        train_r2_per_drug.append(r2_score(Y_train[drug], Y_pred_train_df[drug]))\n        test_r2_per_drug.append(r2_score(Y_test[drug], Y_pred_test_df[drug]))\n        test_mse_per_drug.append(mean_squared_error(Y_test[drug], Y_pred_test_df[drug]))\n    \n    # Aggregate metrics\n    results = {\n        'train_r2_mean': np.mean(train_r2_per_drug),\n        'train_r2_median': np.median(train_r2_per_drug),\n        'test_r2_mean': np.mean(test_r2_per_drug),\n        'test_r2_median': np.median(test_r2_per_drug),\n        'test_mse_mean': np.mean(test_mse_per_drug),\n        'test_r2_positive_frac': np.mean(np.array(test_r2_per_drug) > 0),\n        'per_drug_test_r2': dict(zip(Y_train.columns, test_r2_per_drug)),\n        'per_drug_test_mse': dict(zip(Y_train.columns, test_mse_per_drug)),\n    }\n    \n    return results\n\ndef print_multitask_results(results, model_name):\n    \"\"\"Pretty print multi-task evaluation results.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Model: {model_name}\")\n    print(f\"{'='*60}\")\n    print(f\"Training R² (mean across drugs): {results['train_r2_mean']:.4f}\")\n    print(f\"Training R² (median): {results['train_r2_median']:.4f}\")\n    print(f\"\\nTest R² (mean across drugs): {results['test_r2_mean']:.4f}\")\n    print(f\"Test R² (median): {results['test_r2_median']:.4f}\")\n    print(f\"Test MSE (mean): {results['test_mse_mean']:.4f}\")\n    print(f\"\\nDrugs with positive test R²: {results['test_r2_positive_frac']*100:.1f}%\")\n    \n    # Show best and worst predicted drugs\n    sorted_drugs = sorted(results['per_drug_test_r2'].items(), key=lambda x: x[1], reverse=True)\n    print(f\"\\nTop 5 best predicted drugs:\")\n    for drug, r2 in sorted_drugs[:5]:\n        print(f\"  {drug}: R²={r2:.4f}\")\n    print(f\"\\nTop 5 worst predicted drugs:\")\n    for drug, r2 in sorted_drugs[-5:]:\n        print(f\"  {drug}: R²={r2:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nreeaq95np",
   "source": "---\n# Split Strategy 0: Random Split (Baseline)\n\n**Goal**: Establish baseline performance with standard random train/test split.\n\nThis is the \"easy\" setting because:\n- Test samples are randomly selected, likely similar to training samples\n- Same cancer types and tissues appear in both train and test\n- Model can leverage superficial patterns that don't generalize",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nf15q3kj9z",
   "source": "from sklearn.model_selection import train_test_split\n\n# Standard 80/20 random split\nTEST_SIZE = 0.20\nX_train_rand, X_test_rand, Y_train_rand, Y_test_rand = train_test_split(\n    X, Y_imputed, test_size=TEST_SIZE, random_state=42\n)\n\nprint(f\"Random split ({(1-TEST_SIZE)*100:.0f}/{TEST_SIZE*100:.0f}):\")\nprint(f\"  Train samples: {len(X_train_rand)} ({100*len(X_train_rand)/len(X):.1f}%)\")\nprint(f\"  Test samples: {len(X_test_rand)} ({100*len(X_test_rand)/len(X):.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1cfhiyy5xu",
   "source": "# Scale and PCA for random split\nscaler_rand = RobustScaler()\nX_train_rand_scaled = scaler_rand.fit_transform(X_train_rand)\nX_test_rand_scaled = scaler_rand.transform(X_test_rand)\n\nN_COMPONENTS = 50  # More components for multi-task (capturing more variance)\npca_rand = PCA(n_components=N_COMPONENTS)\nX_train_rand_pca = pca_rand.fit_transform(X_train_rand_scaled)\nX_test_rand_pca = pca_rand.transform(X_test_rand_scaled)\n\nprint(f\"PCA variance explained: {pca_rand.explained_variance_ratio_.sum()*100:.1f}%\")\nprint(f\"X_train shape after PCA: {X_train_rand_pca.shape}\")\nprint(f\"X_test shape after PCA: {X_test_rand_pca.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "916srhs67wh",
   "source": "## Model Training: Random Split (Baseline)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3e1fxxlkulw",
   "source": "# Initialize benchmark results for random split\nbenchmark_rand = pd.DataFrame(columns=[\n    'Model', 'Train R² (mean)', 'Train R² (median)', \n    'Test R² (mean)', 'Test R² (median)', 'Test MSE (mean)', '% Drugs R²>0'\n])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ka9o2cuwozk",
   "source": "from sklearn.linear_model import Ridge\n\n# Ridge Regression (Random Split)\nridge_rand = Ridge(alpha=1.0)\nridge_rand.fit(X_train_rand_pca, Y_train_rand)\n\nresults_ridge_rand = evaluate_multitask(ridge_rand, X_train_rand_pca, Y_train_rand, X_test_rand_pca, Y_test_rand)\nprint_multitask_results(results_ridge_rand, 'Ridge Regression (Random Split)')\n\nbenchmark_rand.loc[len(benchmark_rand)] = [\n    'Ridge', results_ridge_rand['train_r2_mean'], results_ridge_rand['train_r2_median'],\n    results_ridge_rand['test_r2_mean'], results_ridge_rand['test_r2_median'],\n    results_ridge_rand['test_mse_mean'], results_ridge_rand['test_r2_positive_frac']*100\n]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "utxzdiv3u8h",
   "source": "from sklearn.ensemble import RandomForestRegressor\n\n# Random Forest (Random Split)\nrf_rand = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\nrf_rand.fit(X_train_rand_pca, Y_train_rand)\n\nresults_rf_rand = evaluate_multitask(rf_rand, X_train_rand_pca, Y_train_rand, X_test_rand_pca, Y_test_rand)\nprint_multitask_results(results_rf_rand, 'Random Forest (Random Split)')\n\nbenchmark_rand.loc[len(benchmark_rand)] = [\n    'Random Forest', results_rf_rand['train_r2_mean'], results_rf_rand['train_r2_median'],\n    results_rf_rand['test_r2_mean'], results_rf_rand['test_r2_median'],\n    results_rf_rand['test_mse_mean'], results_rf_rand['test_r2_positive_frac']*100\n]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8696p0lylpb",
   "source": "from xgboost import XGBRegressor\n\n# XGBoost (Random Split)\nxgb_base_rand = XGBRegressor(\n    n_estimators=100, \n    max_depth=5, \n    learning_rate=0.1,\n    random_state=42, \n    tree_method='hist', \n    device='cuda',\n    verbosity=0\n)\nxgb_multi_rand = MultiOutputRegressor(xgb_base_rand, n_jobs=1)\n\nprint(f\"Training XGBoost for {len(selected_drugs)} drugs (GPU-accelerated)...\")\nxgb_multi_rand.fit(X_train_rand_pca, Y_train_rand)\n\nresults_xgb_rand = evaluate_multitask(xgb_multi_rand, X_train_rand_pca, Y_train_rand, X_test_rand_pca, Y_test_rand)\nprint_multitask_results(results_xgb_rand, 'XGBoost (Random Split)')\n\nbenchmark_rand.loc[len(benchmark_rand)] = [\n    'XGBoost', results_xgb_rand['train_r2_mean'], results_xgb_rand['train_r2_median'],\n    results_xgb_rand['test_r2_mean'], results_xgb_rand['test_r2_median'],\n    results_xgb_rand['test_mse_mean'], results_xgb_rand['test_r2_positive_frac']*100\n]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "84mm843i0qu",
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"BENCHMARK RESULTS: RANDOM SPLIT (Baseline)\")\nprint(f\"Train: {len(X_train_rand)} samples | Test: {len(X_test_rand)} samples\")\nprint(f\"Drugs: {len(selected_drugs)} | PCA components: {N_COMPONENTS}\")\nprint(\"=\"*70)\ndisplay(benchmark_rand)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "split-strategy-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Split Strategy 1: Histology-Based (Unseen Cancer Subtypes)\n",
    "\n",
    "**Goal**: Test model's ability to predict drug response for cancer subtypes never seen during training.\n",
    "\n",
    "This is a challenging test because:\n",
    "- Different cancer subtypes have distinct molecular profiles\n",
    "- A rare cancer like chondrosarcoma may have completely different drug sensitivities than common cancers like lung adenocarcinoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "histology-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histology distribution\n",
    "histology = df_filtered['primary histology']\n",
    "hist_counts = histology.value_counts(ascending=True)\n",
    "\n",
    "print(f\"Histology types: {len(hist_counts)}\")\n",
    "print(f\"\\nDistribution (ascending by count):\")\n",
    "print(hist_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "histology-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by histology: rare types go to test\n",
    "HISTOLOGY_SPLIT_THRESHOLD = 0.40  # 40% of histology types go to test\n",
    "\n",
    "split_idx = int(len(hist_counts) * HISTOLOGY_SPLIT_THRESHOLD)\n",
    "test_histologies = hist_counts.index[:split_idx].tolist()  # Rarest types\n",
    "train_histologies = hist_counts.index[split_idx:].tolist()  # Common types\n",
    "\n",
    "print(f\"Histology split at {HISTOLOGY_SPLIT_THRESHOLD*100:.0f}% threshold:\")\n",
    "print(f\"  Test histologies: {len(test_histologies)} types (rare, unseen)\")\n",
    "print(f\"  Train histologies: {len(train_histologies)} types (common)\")\n",
    "\n",
    "# Create train/test masks\n",
    "train_mask_hist = histology.isin(train_histologies)\n",
    "test_mask_hist = histology.isin(test_histologies)\n",
    "\n",
    "X_train_hist = X[train_mask_hist]\n",
    "X_test_hist = X[test_mask_hist]\n",
    "Y_train_hist = Y_imputed[train_mask_hist]\n",
    "Y_test_hist = Y_imputed[test_mask_hist]\n",
    "\n",
    "print(f\"\\nTrain samples: {len(X_train_hist)} ({100*len(X_train_hist)/len(X):.1f}%)\")\n",
    "print(f\"Test samples: {len(X_test_hist)} ({100*len(X_test_hist)/len(X):.1f}%)\")\n",
    "print(f\"\\nTest histology types: {test_histologies[:10]}...\" if len(test_histologies) > 10 else f\"\\nTest histology types: {test_histologies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-header",
   "metadata": {},
   "source": [
    "## Feature Scaling and Dimensionality Reduction\n",
    "\n",
    "**Critical**: Methylation (0-1) and expression (2-14) have different scales. Must scale before PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-hist",
   "metadata": {},
   "outputs": [],
   "source": "# Scale features (critical: methylation 0-1, expression 2-14)\nscaler_hist = RobustScaler()\nX_train_hist_scaled = scaler_hist.fit_transform(X_train_hist)\nX_test_hist_scaled = scaler_hist.transform(X_test_hist)\n\n# Dimensionality reduction with PCA (using same N_COMPONENTS as random split)\npca_hist = PCA(n_components=N_COMPONENTS)\nX_train_hist_pca = pca_hist.fit_transform(X_train_hist_scaled)\nX_test_hist_pca = pca_hist.transform(X_test_hist_scaled)\n\nprint(f\"PCA variance explained: {pca_hist.explained_variance_ratio_.sum()*100:.1f}%\")\nprint(f\"X_train shape after PCA: {X_train_hist_pca.shape}\")\nprint(f\"X_test shape after PCA: {X_test_hist_pca.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Model Training: Histology-Based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark results\n",
    "benchmark_hist = pd.DataFrame(columns=[\n",
    "    'Model', 'Train R² (mean)', 'Train R² (median)', \n",
    "    'Test R² (mean)', 'Test R² (median)', 'Test MSE (mean)', '% Drugs R²>0'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ridge-header",
   "metadata": {},
   "source": [
    "### Ridge Regression (Multi-Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ridge-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge regression handles multi-output natively\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_hist_pca, Y_train_hist)\n",
    "\n",
    "results_ridge = evaluate_multitask(ridge, X_train_hist_pca, Y_train_hist, X_test_hist_pca, Y_test_hist)\n",
    "print_multitask_results(results_ridge, 'Ridge Regression')\n",
    "\n",
    "benchmark_hist.loc[len(benchmark_hist)] = [\n",
    "    'Ridge', results_ridge['train_r2_mean'], results_ridge['train_r2_median'],\n",
    "    results_ridge['test_r2_mean'], results_ridge['test_r2_median'],\n",
    "    results_ridge['test_mse_mean'], results_ridge['test_r2_positive_frac']*100\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf-header",
   "metadata": {},
   "source": [
    "### Random Forest (Multi-Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# RandomForest handles multi-output natively\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_hist_pca, Y_train_hist)\n",
    "\n",
    "results_rf = evaluate_multitask(rf, X_train_hist_pca, Y_train_hist, X_test_hist_pca, Y_test_hist)\n",
    "print_multitask_results(results_rf, 'Random Forest')\n",
    "\n",
    "benchmark_hist.loc[len(benchmark_hist)] = [\n",
    "    'Random Forest', results_rf['train_r2_mean'], results_rf['train_r2_median'],\n",
    "    results_rf['test_r2_mean'], results_rf['test_r2_median'],\n",
    "    results_rf['test_mse_mean'], results_rf['test_r2_positive_frac']*100\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-header",
   "metadata": {},
   "source": [
    "### XGBoost (Multi-Output via MultiOutputRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# XGBoost needs MultiOutputRegressor wrapper for multi-output\n",
    "# Note: This trains a separate XGBoost per drug - computationally intensive\n",
    "xgb_base = XGBRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=5, \n",
    "    learning_rate=0.1,\n",
    "    random_state=42, \n",
    "    tree_method='hist', \n",
    "    device='cuda',\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_multi = MultiOutputRegressor(xgb_base, n_jobs=1)  # n_jobs=1 since GPU handles parallelism\n",
    "\n",
    "print(f\"Training XGBoost for {len(selected_drugs)} drugs (GPU-accelerated)...\")\n",
    "xgb_multi.fit(X_train_hist_pca, Y_train_hist)\n",
    "\n",
    "results_xgb = evaluate_multitask(xgb_multi, X_train_hist_pca, Y_train_hist, X_test_hist_pca, Y_test_hist)\n",
    "print_multitask_results(results_xgb, 'XGBoost (Multi-Output)')\n",
    "\n",
    "benchmark_hist.loc[len(benchmark_hist)] = [\n",
    "    'XGBoost', results_xgb['train_r2_mean'], results_xgb['train_r2_median'],\n",
    "    results_xgb['test_r2_mean'], results_xgb['test_r2_median'],\n",
    "    results_xgb['test_mse_mean'], results_xgb['test_r2_positive_frac']*100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-benchmark-hist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK RESULTS: HISTOLOGY-BASED SPLIT (Unseen Cancer Subtypes)\")\n",
    "print(f\"Train: {len(X_train_hist)} samples | Test: {len(X_test_hist)} samples\")\n",
    "print(f\"Drugs: {len(selected_drugs)} | PCA components: {N_COMPONENTS}\")\n",
    "print(\"=\"*70)\n",
    "display(benchmark_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "site-split-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Split Strategy 2: Primary Site-Based (Unseen Tissue Origins)\n",
    "\n",
    "**Goal**: Test model's ability to predict drug response for tissues never seen during training.\n",
    "\n",
    "This tests a different kind of generalization:\n",
    "- Can a model trained on lung/blood cancers predict for pancreas/thyroid cancers?\n",
    "- Different organs have distinct gene expression and methylation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "site-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary site distribution\n",
    "primary_site = df_filtered['primary site']\n",
    "site_counts = primary_site.value_counts(ascending=True)\n",
    "\n",
    "print(f\"Primary sites: {len(site_counts)}\")\n",
    "print(f\"\\nDistribution (ascending by count):\")\n",
    "print(site_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "site-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by primary site: rare tissues go to test\n",
    "SITE_SPLIT_THRESHOLD = 0.40  # 40% of sites go to test\n",
    "\n",
    "site_split_idx = int(len(site_counts) * SITE_SPLIT_THRESHOLD)\n",
    "test_sites = site_counts.index[:site_split_idx].tolist()  # Rarest sites\n",
    "train_sites = site_counts.index[site_split_idx:].tolist()  # Common sites\n",
    "\n",
    "print(f\"Site split at {SITE_SPLIT_THRESHOLD*100:.0f}% threshold:\")\n",
    "print(f\"  Test sites: {len(test_sites)} ({test_sites})\")\n",
    "print(f\"  Train sites: {len(train_sites)} ({train_sites})\")\n",
    "\n",
    "# Create train/test masks\n",
    "train_mask_site = primary_site.isin(train_sites)\n",
    "test_mask_site = primary_site.isin(test_sites)\n",
    "\n",
    "X_train_site = X[train_mask_site]\n",
    "X_test_site = X[test_mask_site]\n",
    "Y_train_site = Y_imputed[train_mask_site]\n",
    "Y_test_site = Y_imputed[test_mask_site]\n",
    "\n",
    "print(f\"\\nTrain samples: {len(X_train_site)} ({100*len(X_train_site)/len(X):.1f}%)\")\n",
    "print(f\"Test samples: {len(X_test_site)} ({100*len(X_test_site)/len(X):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-site",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and PCA for site-based split\n",
    "scaler_site = RobustScaler()\n",
    "X_train_site_scaled = scaler_site.fit_transform(X_train_site)\n",
    "X_test_site_scaled = scaler_site.transform(X_test_site)\n",
    "\n",
    "pca_site = PCA(n_components=N_COMPONENTS)\n",
    "X_train_site_pca = pca_site.fit_transform(X_train_site_scaled)\n",
    "X_test_site_pca = pca_site.transform(X_test_site_scaled)\n",
    "\n",
    "print(f\"PCA variance explained: {pca_site.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"X_train shape after PCA: {X_train_site_pca.shape}\")\n",
    "print(f\"X_test shape after PCA: {X_test_site_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "site-models-header",
   "metadata": {},
   "source": [
    "## Model Training: Site-Based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-site-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark results for site-based split\n",
    "benchmark_site = pd.DataFrame(columns=[\n",
    "    'Model', 'Train R² (mean)', 'Train R² (median)', \n",
    "    'Test R² (mean)', 'Test R² (median)', 'Test MSE (mean)', '% Drugs R²>0'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ridge-site",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "ridge_site = Ridge(alpha=1.0)\n",
    "ridge_site.fit(X_train_site_pca, Y_train_site)\n",
    "\n",
    "results_ridge_site = evaluate_multitask(ridge_site, X_train_site_pca, Y_train_site, X_test_site_pca, Y_test_site)\n",
    "print_multitask_results(results_ridge_site, 'Ridge Regression (Site Split)')\n",
    "\n",
    "benchmark_site.loc[len(benchmark_site)] = [\n",
    "    'Ridge', results_ridge_site['train_r2_mean'], results_ridge_site['train_r2_median'],\n",
    "    results_ridge_site['test_r2_mean'], results_ridge_site['test_r2_median'],\n",
    "    results_ridge_site['test_mse_mean'], results_ridge_site['test_r2_positive_frac']*100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf-site",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_site = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_site.fit(X_train_site_pca, Y_train_site)\n",
    "\n",
    "results_rf_site = evaluate_multitask(rf_site, X_train_site_pca, Y_train_site, X_test_site_pca, Y_test_site)\n",
    "print_multitask_results(results_rf_site, 'Random Forest (Site Split)')\n",
    "\n",
    "benchmark_site.loc[len(benchmark_site)] = [\n",
    "    'Random Forest', results_rf_site['train_r2_mean'], results_rf_site['train_r2_median'],\n",
    "    results_rf_site['test_r2_mean'], results_rf_site['test_r2_median'],\n",
    "    results_rf_site['test_mse_mean'], results_rf_site['test_r2_positive_frac']*100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-site",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_base_site = XGBRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=5, \n",
    "    learning_rate=0.1,\n",
    "    random_state=42, \n",
    "    tree_method='hist', \n",
    "    device='cuda',\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_multi_site = MultiOutputRegressor(xgb_base_site, n_jobs=1)\n",
    "\n",
    "print(f\"Training XGBoost for {len(selected_drugs)} drugs (GPU-accelerated)...\")\n",
    "xgb_multi_site.fit(X_train_site_pca, Y_train_site)\n",
    "\n",
    "results_xgb_site = evaluate_multitask(xgb_multi_site, X_train_site_pca, Y_train_site, X_test_site_pca, Y_test_site)\n",
    "print_multitask_results(results_xgb_site, 'XGBoost (Site Split)')\n",
    "\n",
    "benchmark_site.loc[len(benchmark_site)] = [\n",
    "    'XGBoost', results_xgb_site['train_r2_mean'], results_xgb_site['train_r2_median'],\n",
    "    results_xgb_site['test_r2_mean'], results_xgb_site['test_r2_median'],\n",
    "    results_xgb_site['test_mse_mean'], results_xgb_site['test_r2_positive_frac']*100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-benchmark-site",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK RESULTS: SITE-BASED SPLIT (Unseen Tissue Origins)\")\n",
    "print(f\"Train: {len(X_train_site)} samples | Test: {len(X_test_site)} samples\")\n",
    "print(f\"Drugs: {len(selected_drugs)} | PCA components: {N_COMPONENTS}\")\n",
    "print(\"=\"*70)\n",
    "display(benchmark_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Comparison: Split Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-comparison",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"MULTI-TASK DRUG RESPONSE PREDICTION: FINAL COMPARISON\")\nprint(\"=\"*80)\n\nprint(f\"\\nDataset: {len(X)} cell lines × {len(selected_drugs)} drugs\")\nprint(f\"Features: {X.shape[1]} (methylation + expression) → {N_COMPONENTS} PCA components\")\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"RANDOM SPLIT (Baseline - Easy)\")\nprint(f\"Train: {len(X_train_rand)} | Test: {len(X_test_rand)} ({100*len(X_test_rand)/len(X):.1f}%)\")\nprint(\"-\"*80)\ndisplay(benchmark_rand)\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"HISTOLOGY-BASED SPLIT (Hard - Unseen Cancer Subtypes)\")\nprint(f\"Train: {len(X_train_hist)} | Test: {len(X_test_hist)} ({100*len(X_test_hist)/len(X):.1f}%)\")\nprint(\"-\"*80)\ndisplay(benchmark_hist)\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"SITE-BASED SPLIT (Hard - Unseen Tissue Origins)\")\nprint(f\"Train: {len(X_train_site)} | Test: {len(X_test_site)} ({100*len(X_test_site)/len(X):.1f}%)\")\nprint(\"-\"*80)\ndisplay(benchmark_site)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"INTERPRETATION\")\nprint(\"=\"*80)\nprint(\"\"\"\nComparing split strategies reveals model generalization capability:\n\n1. RANDOM SPLIT (Baseline):\n   - Test samples are similar to training samples\n   - Higher R² expected - this is the \"easy\" setting\n   - If R² is low even here, the task is fundamentally hard\n\n2. HISTOLOGY SPLIT (Hard):\n   - Tests generalization to UNSEEN CANCER SUBTYPES\n   - Drop in R² from baseline shows subtype-specific patterns\n\n3. SITE SPLIT (Hard):\n   - Tests generalization to UNSEEN TISSUE ORIGINS\n   - Drop in R² from baseline shows tissue-specific patterns\n\n4. KEY METRIC: '% Drugs R²>0'\n   - What fraction of drugs can the model predict better than baseline?\n   - Compare across splits to see which drugs are robust vs tissue-specific\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "per-drug-analysis-header",
   "metadata": {},
   "source": [
    "## Per-Drug Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-drug-analysis",
   "metadata": {},
   "outputs": [],
   "source": "# Compare per-drug R² between split strategies (using XGBoost results)\nimport matplotlib.pyplot as plt\n\n# Get per-drug R² from XGBoost for all three splits\nrand_r2 = pd.Series(results_xgb_rand['per_drug_test_r2'])\nhist_r2 = pd.Series(results_xgb['per_drug_test_r2'])\nsite_r2 = pd.Series(results_xgb_site['per_drug_test_r2'])\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Histogram of per-drug R² for all splits\naxes[0].hist(rand_r2, bins=30, alpha=0.7, label='Random Split', color='green')\naxes[0].hist(hist_r2, bins=30, alpha=0.7, label='Histology Split', color='blue')\naxes[0].hist(site_r2, bins=30, alpha=0.7, label='Site Split', color='orange')\naxes[0].axvline(x=0, color='red', linestyle='--', label='Baseline (R²=0)')\naxes[0].set_xlabel('Test R² per Drug')\naxes[0].set_ylabel('Number of Drugs')\naxes[0].set_title('Distribution of Per-Drug Test R²')\naxes[0].legend()\n\n# Scatter: Random R² vs Histology R²\naxes[1].scatter(rand_r2, hist_r2, alpha=0.5, color='blue')\naxes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\naxes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5)\naxes[1].plot([-0.5, 0.5], [-0.5, 0.5], 'k--', alpha=0.3, label='y=x')\naxes[1].set_xlabel('Test R² (Random Split)')\naxes[1].set_ylabel('Test R² (Histology Split)')\naxes[1].set_title('Random vs Histology Split')\naxes[1].legend()\n\n# Scatter: Random R² vs Site R²\naxes[2].scatter(rand_r2, site_r2, alpha=0.5, color='orange')\naxes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\naxes[2].axvline(x=0, color='red', linestyle='--', alpha=0.5)\naxes[2].plot([-0.5, 0.5], [-0.5, 0.5], 'k--', alpha=0.3, label='y=x')\naxes[2].set_xlabel('Test R² (Random Split)')\naxes[2].set_ylabel('Test R² (Site Split)')\naxes[2].set_title('Random vs Site Split')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\nPerformance degradation from Random (baseline) to harder splits:\")\nprint(f\"  Random → Histology: {(rand_r2 - hist_r2).mean():.4f} mean R² drop\")\nprint(f\"  Random → Site: {(rand_r2 - site_r2).mean():.4f} mean R² drop\")\nprint(f\"\\nDrugs with R²>0 by split:\")\nprint(f\"  Random: {(rand_r2 > 0).sum()} / {len(rand_r2)} ({100*(rand_r2 > 0).mean():.1f}%)\")\nprint(f\"  Histology: {(hist_r2 > 0).sum()} / {len(hist_r2)} ({100*(hist_r2 > 0).mean():.1f}%)\")\nprint(f\"  Site: {(site_r2 > 0).sum()} / {len(site_r2)} ({100*(site_r2 > 0).mean():.1f}%)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}